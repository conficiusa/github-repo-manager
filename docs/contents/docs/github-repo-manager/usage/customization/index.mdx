---
title: Customization
description: This guide provides advanced usage techniques and examples for GitHub Repository Manager to help you maximize its capabilities.
keywords: ["customization", "advanced", "examples", "scripting", "automation"]
---

## Scripting with GitHub Repository Manager

You can integrate GitHub Repository Manager into your Python scripts for automation or custom workflows.

### Example: Automated Cleanup Script

```python
#!/usr/bin/env python3
"""
Example script to automatically clean up test repositories older than 30 days
"""
from github_repo_manager import GitHubRepoManager
from datetime import datetime, timedelta
import os

# Initialize with environment variables
manager = GitHubRepoManager()

# Get all repositories
all_repos = manager.get_repositories()

# Filter repositories based on custom criteria
thirty_days_ago = datetime.now() - timedelta(days=30)
old_test_repos = [
    repo for repo in all_repos
    if (repo['name'].startswith('test-') and 
        datetime.strptime(repo['created_at'], '%Y-%m-%dT%H:%M:%SZ') < thirty_days_ago)
]

print(f"Found {len(old_test_repos)} test repositories older than 30 days")

# Delete the repositories with confirmation
if old_test_repos:
    manager.bulk_delete(
        old_test_repos,
        dry_run=False,
        confirm=True,
        delay=1.0
    )
```

## Working with Repository Data

The GitHub API returns rich repository data that you can use for advanced filtering or processing:

```python
from github_repo_manager import GitHubRepoManager
import json

# Initialize the manager
manager = GitHubRepoManager()

# Get repositories
repos = manager.get_repositories()

# Example: Find dormant repositories (no updates in past year)
from datetime import datetime, timedelta
one_year_ago = datetime.now() - timedelta(days=365)

dormant_repos = [
    repo for repo in repos
    if datetime.strptime(repo['updated_at'], '%Y-%m-%dT%H:%M:%SZ') < one_year_ago
]

print(f"Found {len(dormant_repos)} dormant repositories:")
for repo in dormant_repos:
    print(f"- {repo['name']} (Last updated: {repo['updated_at']})")
```

## Creating a Custom Interactive Tool

You can build custom tools on top of GitHub Repository Manager:

```python
#!/usr/bin/env python3
"""
Example: Custom repository categorization tool
"""
from github_repo_manager import GitHubRepoManager
import os
import json

def categorize_repositories():
    manager = GitHubRepoManager()
    repos = manager.get_repositories()
    
    # Create categories
    categories = {
        "active": [],
        "archived": [],
        "forks": [],
        "personal": [],
        "other": []
    }
    
    # Categorize repositories
    for repo in repos:
        if repo['fork']:
            categories["forks"].append(repo)
        elif repo['archived']:
            categories["archived"].append(repo)
        elif repo['name'].startswith('personal-'):
            categories["personal"].append(repo)
        elif repo['updated_at'] > (datetime.now() - timedelta(days=90)).isoformat():
            categories["active"].append(repo)
        else:
            categories["other"].append(repo)
    
    # Display summary
    print("Repository Categories:")
    for category, repo_list in categories.items():
        print(f"- {category}: {len(repo_list)} repositories")
    
    # Save to file for reference
    with open('repo_categories.json', 'w') as f:
        json.dump(
            {cat: [r['name'] for r in repos] for cat, repos in categories.items()},
            f, indent=2
        )
    
    print("\nCategories saved to repo_categories.json")

if __name__ == "__main__":
    categorize_repositories()
```

## Integration with Other Tools

### Example: GitHub Repository Manager in a Cleanup Pipeline

```bash
#!/bin/bash
# Script to perform weekly repository maintenance

# 1. Run repository analysis
python analyze_repos.py > repo_stats.json

# 2. Archive inactive repositories
github-repo-manager --pattern "$(jq -r '.inactive_repos[]' repo_stats.json | tr '\n' '|')" --archive --no-confirm

# 3. Delete temporary repositories older than 30 days
github-repo-manager --pattern "temp-" --created-before "30 days ago" --no-confirm

# 4. Log the results
echo "Repository cleanup completed on $(date)" >> cleanup_log.txt
```

## Extending the Core Functionality

You can extend GitHub Repository Manager by subclassing its main class:

```python
from github_repo_manager import GitHubRepoManager
import requests

class EnhancedRepoManager(GitHubRepoManager):
    def __init__(self, token=None, username=None):
        super().__init__(token, username)
    
    def archive_repository(self, repo_name):
        """Archive instead of delete a repository"""
        url = f"{self.base_url}/repos/{self.username}/{repo_name}"
        data = {"archived": True}
        response = requests.patch(url, headers=self.headers, json=data)
        
        if response.status_code == 200:
            print(f"Successfully archived {repo_name}")
            return True
        else:
            print(f"Failed to archive {repo_name}: {response.status_code}")
            return False
    
    def bulk_archive(self, repos, dry_run=True, confirm=True, delay=1.0):
        """Bulk archive repositories with the same interface as bulk_delete"""
        # Get selected repos with the interactive UI
        if confirm:
            selected_repos = self.select_repositories_interactively(repos)
        else:
            selected_repos = repos
            
        if dry_run:
            print("\nDRY RUN: No repositories were archived.")
            return 0
                
        # Archive repositories
        archived_count = 0
        for repo in selected_repos:
            if self.archive_repository(repo['name']):
                archived_count += 1
            if delay > 0 and repo != selected_repos[-1]:
                time.sleep(delay)
                
        print(f"\nOperation completed. Archived {archived_count} repositories.")
        return archived_count
```

## Custom Filtering Examples

Create more complex filters for your repositories:

```python
from github_repo_manager import GitHubRepoManager
import re

manager = GitHubRepoManager()
repos = manager.get_repositories()

# Find empty repositories
empty_repos = [repo for repo in repos if repo['size'] == 0]

# Find repositories with non-standard licenses
non_standard_license_repos = [
    repo for repo in repos 
    if 'license' not in repo or repo['license'] is None
]

# Find repositories using a specific language
python_repos = [
    repo for repo in repos
    if repo['language'] == 'Python'
]
```

## Tips for Large-Scale Operations

When working with hundreds or thousands of repositories:

1. **Use pagination efficiently**:
   ```python
   # Get all repositories with custom pagination
   all_repos = []
   page = 1
   while True:
       repos_page = manager.get_repositories(max_pages=1, page=page)
       if not repos_page:
           break
       all_repos.extend(repos_page)
       page += 1
   ```

2. **Process repositories in batches**:
   ```python
   # Process repositories in batches of 50
   batch_size = 50
   for i in range(0, len(repos), batch_size):
       batch = repos[i:i+batch_size]
       # Process this batch
       manager.bulk_delete(batch, dry_run=True)
   ```

3. **Implement error handling**:
   ```python
   # Example with error handling and retry logic
   max_retries = 3
   for repo in selected_repos:
       retries = 0
       while retries < max_retries:
           try:
               success = manager.delete_repository(repo['name'])
               if success:
                   break
           except Exception as e:
               print(f"Error: {str(e)}")
               retries += 1
               if retries < max_retries:
                   print(f"Retrying ({retries}/{max_retries})...")
                   time.sleep(2)  # Wait before retry
   ```

## Best Practices for Advanced Use

1. **Test with dry runs**: Always test your scripts with `dry_run=True` before executing actual operations.

2. **Error logging**: Implement proper error logging for automated scripts:
   ```python
   import logging
   logging.basicConfig(
       filename='repo_operations.log',
       level=logging.INFO,
       format='%(asctime)s - %(levelname)s - %(message)s'
   )
   
   try:
       # Operations here
       logging.info("Successfully processed repositories")
   except Exception as e:
       logging.error(f"Error during processing: {str(e)}")
   ```

3. **Incremental operations**: For large accounts, consider incremental operations:
   ```python
   # Save state between runs
   import json
   
   def save_progress(processed_repos):
       with open('progress.json', 'w') as f:
           json.dump(processed_repos, f)
   
   def load_progress():
       try:
           with open('progress.json', 'r') as f:
               return json.load(f)
       except (FileNotFoundError, json.JSONDecodeError):
           return []
   
   # Resume from last run
   already_processed = load_progress()
   repos_to_process = [r for r in repos if r['name'] not in already_processed]
   ```